import torch
import torch.nn as nn
import math
from einops import rearrange

class HierarchicalProjector(nn.Module):
    """
    åˆ†å±‚å­¦ä¹ çš„ patch å‹ç¼©å™¨
    196 patches -> 49 patchesï¼Œ384 dim -> 64 dim
    """
    def __init__(
        self,
        in_features,
        out_features,
        input_patches=196,  # 14x14
        output_patches=49,  # 7x7
        num_layers=3,
        hidden_dim=256,
        use_cross_attention=True
    ):
        super().__init__()

        self.input_patches = input_patches
        self.output_patches = output_patches
        self.in_h = self.in_w = int(math.sqrt(input_patches))  # 14
        self.out_h = self.out_w = int(math.sqrt(output_patches))  # 7

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(in_features, hidden_dim)

        # åˆ†å±‚ä¸‹é‡‡æ ·ç¼–ç å™¨
        self.encoder_layers = nn.ModuleList()

        # ç¬¬ä¸€å±‚: 14x14 -> 7x7 (4:1 reduction)
        self.encoder_layers.append(
            PatchDownsampleBlock(
                hidden_dim, hidden_dim,
                downsample_ratio=2,  # 2x2 -> 1
                use_attention=use_cross_attention
            )
        )

        # ä¸­é—´å±‚ï¼šç‰¹å¾ç»†åŒ–
        for _ in range(num_layers - 1):
            self.encoder_layers.append(
                PatchRefinementBlock(
                    hidden_dim, hidden_dim,
                    use_attention=use_cross_attention
                )
            )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, out_features)

        # Layer norm
        self.norm = nn.LayerNorm(out_features)

    def forward(self, x):
        """
        x: (b, t, input_patches, in_features)
        -> (b, t, output_patches, out_features)
        """
        b, t, n_in, d_in = x.shape

        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)  # (b, t, n_in, hidden_dim)

        # Reshape to spatial format
        x = rearrange(x, 'b t (h w) d -> b t h w d', h=self.in_h, w=self.in_w)

        # é€šè¿‡ç¼–ç å™¨å±‚
        for layer in self.encoder_layers:
            x = layer(x)

        # Reshape back to patches
        x = rearrange(x, 'b t h w d -> b t (h w) d')

        # è¾“å‡ºæŠ•å½±å’Œå½’ä¸€åŒ–
        x = self.output_proj(x)
        x = self.norm(x)

        return x


class PatchDownsampleBlock(nn.Module):
    """ä¸‹é‡‡æ · block: å‡å°‘ç©ºé—´åˆ†è¾¨ç‡"""
    def __init__(self, in_dim, out_dim, downsample_ratio=2, use_attention=True):
        super().__init__()
        self.downsample_ratio = downsample_ratio

        if use_attention:
            # ä½¿ç”¨æ³¨æ„åŠ›æ± åŒ–ä¸‹é‡‡æ ·
            self.downsample = AttentionPooling2D(
                in_dim, out_dim,
                kernel_size=downsample_ratio,
                stride=downsample_ratio
            )
        else:
            # ä½¿ç”¨å·ç§¯ä¸‹é‡‡æ ·
            self.downsample = nn.Conv2d(
                in_dim, out_dim,
                kernel_size=downsample_ratio,
                stride=downsample_ratio
            )

        # ç‰¹å¾ç»†åŒ–
        self.refine = nn.Sequential(
            nn.Linear(out_dim, out_dim * 2),
            nn.GELU(),
            nn.Linear(out_dim * 2, out_dim),
            nn.LayerNorm(out_dim)
        )

    def forward(self, x):
        """
        x: (b, t, h, w, d)
        """
        b, t, h, w, d = x.shape

        # Reshape for conv/attention
        x = rearrange(x, 'b t h w d -> (b t) d h w')

        # ä¸‹é‡‡æ ·
        x = self.downsample(x)  # (bt, d, h//r, w//r)

        # Reshape back
        x = rearrange(x, '(b t) d h w -> b t h w d', b=b)

        # ç‰¹å¾ç»†åŒ–
        x = self.refine(x)

        return x


class PatchRefinementBlock(nn.Module):
    """ç‰¹å¾ç»†åŒ– block: ä¿æŒç©ºé—´å°ºå¯¸ï¼Œæ”¹å–„ç‰¹å¾"""
    def __init__(self, dim, hidden_dim=None, use_attention=True):
        super().__init__()
        hidden_dim = hidden_dim or dim * 2

        if use_attention:
            self.attn = SpatialSelfAttention(dim)
        else:
            self.attn = None

        # MLP
        self.mlp = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, dim),
        )

        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)

    def forward(self, x):
        """x: (b, t, h, w, d)"""

        # Self attention
        if self.attn:
            x = x + self.attn(self.norm1(x))

        # MLP
        x = x + self.mlp(self.norm2(x))

        return x


class AttentionPooling2D(nn.Module):
    """2D æ³¨æ„åŠ›æ± åŒ–å±‚"""
    def __init__(self, in_dim, out_dim, kernel_size, stride):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride

        # Query, Key, Value projections
        self.q_proj = nn.Linear(in_dim, out_dim)
        self.k_proj = nn.Linear(in_dim, out_dim)
        self.v_proj = nn.Linear(in_dim, out_dim)

        self.scale = out_dim ** -0.5

    def forward(self, x):
        """
        x: (bt, in_dim, h, w)
        -> (bt, out_dim, h//stride, w//stride)
        """
        bt, d, h, w = x.shape

        # è½¬æ¢ä¸º patch format
        x = rearrange(x, 'bt d h w -> bt (h w) d')

        # åˆ†ç»„ä¸º kernel_size x kernel_size çš„çª—å£
        # ç®€åŒ–å¤„ç†ï¼šä½¿ç”¨å¹³å‡æ± åŒ–çš„çª—å£
        x = rearrange(x, 'bt (h w) d -> bt h w d', h=h, w=w)

        # æŒ‰ stride åˆ†å—
        x = rearrange(x, 'bt (nh kh) (nw kw) d -> bt (nh nw) (kh kw) d',
                     kh=self.kernel_size, kw=self.kernel_size)

        # è®¡ç®—æ³¨æ„åŠ›
        q = self.q_proj(x.mean(dim=2, keepdim=True))  # (bt, patches, 1, out_dim)
        k = self.k_proj(x)  # (bt, patches, kernel_size^2, out_dim)
        v = self.v_proj(x)  # (bt, patches, kernel_size^2, out_dim)

        # Attention
        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn = torch.softmax(attn, dim=-1)

        out = torch.matmul(attn, v).squeeze(2)  # (bt, patches, out_dim)

        # Reshape to spatial
        nh = nw = h // self.stride
        out = rearrange(out, 'bt (nh nw) d -> bt d nh nw', nh=nh, nw=nw)

        return out


class SpatialSelfAttention(nn.Module):
    """ç©ºé—´è‡ªæ³¨æ„åŠ›"""
    def __init__(self, dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.scale = (dim // num_heads) ** -0.5

        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)

    def forward(self, x):
        """x: (b, t, h, w, d)"""
        b, t, h, w, d = x.shape

        # Reshape for attention
        x = rearrange(x, 'b t h w d -> (b t) (h w) d')

        # QKV
        qkv = self.qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, 'bt n (h d) -> bt h n d', h=self.num_heads), qkv)

        # Attention
        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn = torch.softmax(attn, dim=-1)

        out = torch.matmul(attn, v)
        out = rearrange(out, 'bt h n d -> bt n (h d)')
        out = self.proj(out)

        # Reshape back
        out = rearrange(out, '(b t) (h w) d -> b t h w d', b=b, h=h, w=w)

        return out


class HierarchicalDecoder(nn.Module):
    """
    åˆ†å±‚ä¸Šé‡‡æ ·è§£ç å™¨: 49 patches -> 196 patches
    """
    def __init__(
        self,
        in_features,
        out_features,
        input_patches=49,
        output_patches=196,
        hidden_dim=256
    ):
        super().__init__()

        self.input_patches = input_patches
        self.output_patches = output_patches
        self.in_h = self.in_w = int(math.sqrt(input_patches))  # 7
        self.out_h = self.out_w = int(math.sqrt(output_patches))  # 14

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(in_features, hidden_dim)

        # ä¸Šé‡‡æ ·å±‚
        self.upsample = PatchUpsampleBlock(
            hidden_dim, hidden_dim,
            upsample_ratio=2
        )

        # ç‰¹å¾ç»†åŒ–
        self.refine = PatchRefinementBlock(hidden_dim, use_attention=False)

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, out_features)

    def forward(self, x):
        """
        x: (b, t, input_patches, in_features)
        -> (b, t, output_patches, out_features)
        """
        # æŠ•å½±å’Œé‡å¡‘
        x = self.input_proj(x)
        x = rearrange(x, 'b t (h w) d -> b t h w d', h=self.in_h, w=self.in_w)

        # ä¸Šé‡‡æ ·
        x = self.upsample(x)

        # ç»†åŒ–
        x = self.refine(x)

        # è¾“å‡º
        x = rearrange(x, 'b t h w d -> b t (h w) d')
        x = self.output_proj(x)

        return x


class PatchUpsampleBlock(nn.Module):
    """ä¸Šé‡‡æ · block"""
    def __init__(self, in_dim, out_dim, upsample_ratio=2):
        super().__init__()

        self.upsample = nn.ConvTranspose2d(
            in_dim, out_dim,
            kernel_size=upsample_ratio,
            stride=upsample_ratio
        )

        self.refine = nn.Sequential(
            nn.Linear(out_dim, out_dim * 2),
            nn.GELU(),
            nn.Linear(out_dim * 2, out_dim),
            nn.LayerNorm(out_dim)
        )

    def forward(self, x):
        b, t, h, w, d = x.shape

        # è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·
        x = rearrange(x, 'b t h w d -> (b t) d h w')
        x = self.upsample(x)
        x = rearrange(x, '(b t) d h w -> b t h w d', b=b)

        # ç»†åŒ–
        x = self.refine(x)

        return x


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª Testing HierarchicalProjector and HierarchicalDecoder...")
    print("ğŸ“‹ Based on current VWorldModel implementation:")
    print("   - Current: (b, t, num_patches, 394) -> (b, t, num_patches, 64)")
    print("   - Target:  (b, t, 196, 394) -> (b, t, 49, 64)")

    # å‚æ•°è®¾ç½® - åŒ¹é…å½“å‰å®ç°
    batch_size = 2
    time_steps = 3
    input_patches = 196  # 14x14 DINO patches
    output_patches = 49  # 7x7 compressed patches
    in_features = 394  # 384 (visual) + 10 (proprio)
    out_features = 64  # projected features

    # åˆ›å»ºæ¨¡å‹
    encoder = HierarchicalProjector(
        in_features=in_features,
        out_features=out_features,
        input_patches=input_patches,
        output_patches=output_patches,
        num_layers=3,
        hidden_dim=256,
        use_cross_attention=True
    )

    decoder = HierarchicalDecoder(
        in_features=out_features,
        out_features=in_features,
        input_patches=output_patches,
        output_patches=input_patches,
        hidden_dim=256
    )

    print(f"ğŸ“Š Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}")
    print(f"ğŸ“Š Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}")

    # æµ‹è¯• forward pass
    print(f"\nğŸ”¬ Testing forward pass...")
    print(f"Input shape: ({batch_size}, {time_steps}, {input_patches}, {in_features})")

    # åˆ›å»ºéšæœºè¾“å…¥
    x = torch.randn(batch_size, time_steps, input_patches, in_features)
    print(f"âœ… Input tensor created: {x.shape}")

    # Encoder forward
    try:
        encoded = encoder(x)
        print(f"âœ… Encoder output: {encoded.shape}")
        assert encoded.shape == (batch_size, time_steps, output_patches, out_features)
        print(f"âœ… Encoder output shape correct!")
    except Exception as e:
        print(f"âŒ Encoder failed: {e}")
        exit(1)

    # Decoder forward
    try:
        decoded = decoder(encoded)
        print(f"âœ… Decoder output: {decoded.shape}")
        assert decoded.shape == (batch_size, time_steps, input_patches, in_features)
        print(f"âœ… Decoder output shape correct!")
    except Exception as e:
        print(f"âŒ Decoder failed: {e}")
        exit(1)

    # æµ‹è¯•æ¢¯åº¦æµ
    try:
        loss = decoded.sum()
        loss.backward()
        print(f"âœ… Gradients computed successfully!")
    except Exception as e:
        print(f"âŒ Gradient computation failed: {e}")
        exit(1)

    print(f"\nğŸ‰ All tests passed!")
    print(f"ğŸ“ˆ Compression ratio: {input_patches}/{output_patches} = {input_patches/output_patches:.1f}x patches")
    print(f"ğŸ“ˆ Feature compression: {in_features}/{out_features} = {in_features/out_features:.1f}x features")
    print(f"ğŸ“ˆ Total compression: {(input_patches*in_features)/(output_patches*out_features):.1f}x")